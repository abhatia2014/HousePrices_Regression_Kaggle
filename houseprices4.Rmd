
---
title: House Prices - Advanced Regression Techniques4
output: html_notebook
---
In this exercise, we'll try model stacking separately for numeric and factor variables. We'll also use preprocessing
load the required packages first

```{r}
library(mlr,quietly = TRUE)
library(dplyr,quietly = TRUE)
library(ggplot2,quietly = TRUE)
library(caret,quietly = TRUE)
library(purrr,quietly = TRUE)
library(data.table,quietly = TRUE,warn.conflicts = FALSE)
library(readr,quietly = TRUE,warn.conflicts = FALSE)
```

Load the test and train datasets

```{r}
train=read_csv("train.csv")
test=read_csv("test.csv")
```

Rbind the two datasets to perform feature engineering, will later separate the two datasets again. Remove the variable Sale Price from the training set

```{r}
SalePrice=train$SalePrice
train$SalePrice=NULL
train_test=rbind(train,test)
```

Find the str of the combined train_test dataset. Also take the summary

```{r}
str(train_test)
#MSSubClass, OverallQuality, OverallCondition, YearBuilt,YrRemodAdd, GarageYrBlt, MoSold, YrSold are numeric variables that must be converted to factor
summary(train_test)
```

Take all 'Character' class types and convert to factor (categorical) types. Add other numeric variables identified above and convert to factor

```{r}
trn_tst_chr=train_test %>% 
  select_if(is.character) %>% 
  mutate_each(funs(as.factor))
trn_tst_chr$MSSubClass=as.factor(train_test$MSSubClass)
trn_tst_chr$OverallQual=as.factor(train_test$OverallQual)
trn_tst_chr$OverallCond=as.factor(train_test$OverallCond)
trn_tst_chr$MoSold=as.factor(train_test$MoSold)
str(trn_tst_chr)
```

find the number of NAs in each of the factor variables

```{r}
colSums(sapply(trn_tst_chr,is.na))
```

Variables Alley, FireplaceQu, PoolQC, Fence, MiscFeature have very high NAs and are removed

```{r}
trn_tst_chr$Alley=NULL
trn_tst_chr$FireplaceQu=NULL
trn_tst_chr$PoolQC=NULL
trn_tst_chr$Fence=NULL
trn_tst_chr$MiscFeature=NULL
# re-run the command for finding total NAs in the factor variables
colSums(sapply(trn_tst_chr,is.na))
```

All variables with Garage that have missing values should be changed to none, likewise all variablesthat have bmst missing values should also be changed to none. Probably these houses don't have a garage/ basement

```{r}
trn_tst_chr$BsmtQual=as.character(trn_tst_chr$BsmtQual)
trn_tst_chr[is.na(trn_tst_chr$BsmtQual),"BsmtQual"]="none"
trn_tst_chr$BsmtQual=as.factor(trn_tst_chr$BsmtQual)

trn_tst_chr$BsmtCond=as.character(trn_tst_chr$BsmtCond)
trn_tst_chr[is.na(trn_tst_chr$BsmtCond),"BsmtCond"]="none"
trn_tst_chr$BsmtCond=as.factor(trn_tst_chr$BsmtCond)

trn_tst_chr$BsmtExposure=as.character(trn_tst_chr$BsmtExposure)
trn_tst_chr[is.na(trn_tst_chr$BsmtExposure),"BsmtExposure"]="none"
trn_tst_chr$BsmtExposure=as.factor(trn_tst_chr$BsmtExposure)

trn_tst_chr$BsmtFinType1=as.character(trn_tst_chr$BsmtFinType1)
trn_tst_chr[is.na(trn_tst_chr$BsmtFinType1),"BsmtFinType1"]="none"
trn_tst_chr$BsmtFinType1=as.factor(trn_tst_chr$BsmtFinType1)

trn_tst_chr$BsmtFinType1=as.character(trn_tst_chr$BsmtFinType1)
trn_tst_chr[is.na(trn_tst_chr$BsmtFinType1),"BsmtFinType1"]="none"
trn_tst_chr$BsmtFinType1=as.factor(trn_tst_chr$BsmtFinType1)

trn_tst_chr$BsmtFinType2=as.character(trn_tst_chr$BsmtFinType2)
trn_tst_chr[is.na(trn_tst_chr$BsmtFinType2),"BsmtFinType2"]="none"
trn_tst_chr$BsmtFinType2=as.factor(trn_tst_chr$BsmtFinType2)

#impute the other garage types using the impute function of mlr package

imp_fctr=impute(trn_tst_chr,cols = list(GarageType="none",GarageFinish="none",GarageQual="none",GarageCond="none"))
imp_fctr$data

colSums(sapply(imp_fctr$data,is.na))
trn_tst_chr=imp_fctr$data
#for the remaining variables, we impute using impute mode 
imp_fctr=impute(trn_tst_chr,classes = list(factor=imputeMode()))
trn_tst_chr=imp_fctr$data
#check once again for any missing values
colSums(sapply(trn_tst_chr,is.na))
#no missing values now
```

We'll separate out the train_chr data from tst_chr data, reinsert the SalePrice for the train_chr data and do a benchmarking experiment for selecting the best learner for the factor data only

```{r}
train_factor=trn_tst_chr[1:1460,]
test_factor=trn_tst_chr[1461:nrow(trn_tst_chr),]
table(train_factor$MSSubClass)
table(test_factor$MSSubClass)
#empty factor level in train_factor
train_factor[57,"MSSubClass"] = 150
train_factor$SalePrice=SalePrice
#set up a regr task
fact_regr=makeRegrTask(data = train_factor,target="SalePrice")
#set up resampling, in this case CV with 3 iterations
fac_resam=makeResampleDesc(method="CV",iters=3)
#make learner, first get a list of learners
regr_learners=listLearners(fact_regr)
fac_learner=list(makeLearner("regr.glmnet"),makeLearner("regr.ksvm"),makeLearner("regr.cvglmnet"),makeLearner("regr.randomForest"),makeLearner("regr.earth"))
listMeasures(fact_regr)
fac_measures=list(mse,rsq)
#run the benchmark experiment
fac_benchmark=benchmark(learners = fac_learner,tasks = fact_regr,resamplings = fac_resam,measures = fac_measures)
fac_benchmark
#best results from RandomForest and cvglmnet,we train the model using randomForest with default hyperparamters and predict the fac_test dataset

fac_train=mlr::train(learner = "regr.randomForest",task = fact_regr)
fac_train_predict=predict(fac_train,task = fact_regr)
names(fac_train_predict)
fac_train_predict$data
train_factor$pred1=fac_train_predict$data$response
fac_test_predict=predict(fac_train,newdata = test_factor)
fac_test_predict$data
test_factor$pred1=fac_test_predict$data$response
```

Take all numerics variables separately 

```{r}
train_test$MSSubClass=as.factor(train_test$MSSubClass)
train_test$OverallCond=as.factor(train_test$OverallCond)
train_test$OverallQual=as.factor(train_test$OverallQual)
train_test$MoSold=as.factor(train_test$MoSold)
trn_tst_num=sapply(train_test,is.numeric)
trn_tst_numeric=train_test[,trn_tst_num]
#convert year variables to age and remove year variables
library(lubridate)
trn_tst_numeric$builtage=as.numeric(year(today())-trn_tst_numeric$YearBuilt)
trn_tst_numeric$YearBuilt=NULL
trn_tst_numeric$remodelledage=as.numeric(year(today())-trn_tst_numeric$YearRemodAdd)
trn_tst_numeric$YearRemodAdd=NULL
trn_tst_numeric$garageage=as.numeric(year(today())-trn_tst_numeric$GarageYrBlt)
trn_tst_numeric$GarageYrBlt=NULL
trn_tst_numeric$soldAge=as.numeric(year(today())-trn_tst_numeric$YrSold)
trn_tst_numeric$YrSold=NULL
```

Find the number of NAs in the numeric variables

```{r}
colSums(sapply(trn_tst_numeric,is.na))
```

There are not too many columns with many NAs. We can impute the missing values by a measure of centrality - mean or median. For the lot Frontage, there are 486 missing values(~25%)- these have to be imputed as a function of Lot Area. For missing values in Garage age- those will be 0 as they represent houses with no garages. For MasVnrArea and Basement missing values- we substitue 0 

```{r}
#first GarageAge

#next we impute Lot Frontage as a regression linear model function of Lot Area
imp_lotfront=impute(trn_tst_numeric, cols = list(LotFrontage=imputeLearner(makeLearner("regr.lm"),features = "LotArea"),MasVnrArea=0,BsmtFinSF1=0,BsmtFinSF2=0,BsmtUnfSF=0, TotalBsmtSF=0, BsmtFullBath=0, BsmtHalfBath=0, GarageCars=0,GarageArea=0,garageage=0))

imp_lotfront$desc
head(imp_lotfront$data,10)
# again find the total NAs in each variable
colSums(sapply(imp_lotfront$data,is.na))
trn_tst_numeric=imp_lotfront$data
colSums(sapply(trn_tst_numeric,is.na))
# no NAs left in the model
```


Check and remove any constant features (min variation) as they don't help in machine learning, The percentage of variabilitiy is kept at 5% meaning all features with below 5% variability are removed

```{r}
trn_tst_numeric=removeConstantFeatures(trn_tst_numeric,perc = 0.05)

```

5 variables with low variance were removed. 
LowQualFinSF,KitchenAbvGr,X3SsnPorch,PoolArea,MiscVal
Next visualize intra feature correlation. Load package corrplot

```{r}
library(corrplot)

numcorr=cor(trn_tst_numeric)
corrplot(numcorr)
```

Find variables of high correlation with each other (multicollinearity) and remove them. Use findCorrelation function from Caret package. All variables with more than 70% correlation are flagged and removed

```{r}
highcorvar=findCorrelation(numcorr,cutoff = 0.70)
# 3 Variables have high multicollinearity
names(trn_tst_numeric[,highcorvar])
# "GrLivArea"    "X1stFlrSF"    "GarageCars"  are the variables with high multicollinearity
trn_tst_numeric=trn_tst_numeric[,-highcorvar]
```

We'll do a separate machine learning for numerical variables 

```{r}
train_numeric=trn_tst_numeric[1:1460,]
test_numeric=trn_tst_numeric[1461:nrow(trn_tst_numeric),]

train_numeric$SalePrice=SalePrice
#set up a regr task
num_regr=makeRegrTask(data = train_numeric,target="SalePrice")
#set up resampling, in this case CV with 3 iterations
num_resam=makeResampleDesc(method="CV",iters=3)
#make learner, first get a list of learners
regr_learners=listLearners(num_regr)
num_learner=list(makeLearner("regr.glmnet"),makeLearner("regr.ksvm"),makeLearner("regr.glm"),makeLearner("regr.randomForest"),makeLearner("regr.gbm"),makeLearner("regr.earth"))
listMeasures(num_regr)
num_measures=list(mse,rsq)
#run the benchmark experiment
num_benchmark=benchmark(learners = num_learner,tasks = num_regr,resamplings = num_resam,measures = num_measures)
num_benchmark
#best results from RandomForest and earth,we train the model using earth with default hyperparamters and predict the num_test dataset

num_train=mlr::train(learner = "regr.earth",task = num_regr)
num_train_predict=predict(num_train,task = num_regr)

num_train_predict$data
train_numeric$pred1=num_train_predict$data$response
num_test_predict=predict(num_train,newdata = test_numeric)
num_test_predict$data
test_numeric$pred1=num_test_predict$data$response
```

Remove variables that are not needed

```{r}
rm(numcorr,test,train,train_test,trn_tst_chr,trn_tst_num,fac_benchmark,fac_learner,fac_measures,fac_resam,fac_test_predict,fac_train,fact_regr,highcorvar,imp_fctr,imp_lotfront,num_benchmark,num_learner,num_learner,num_measures,num_regr,num_resam,num_test_predict,num_train_predict)
```


combine numeric, and factordatasets back together using cbind, remove other datasets that are no longer needed

```{r}
#change name of prediction variable in the train_factor to facpred
train_factor=train_factor %>% 
  mutate(facpred=pred1)
train_factor$pred1=NULL
#similarly change for test_factor variable
test_factor=test_factor %>% 
  mutate(facpred=pred1)
test_factor$pred1=NULL
# now combine factor with numeric datasets
train_all=cbind(train_numeric,train_factor)
test_all=cbind(test_numeric,test_factor)

```

Now we just select the features pred1, predfac for both train and test all datasets for 2nd level of machine learning

```{r}
names(train_all)
train_select=train_all[,-c(26)]
train_select$Id=NULL
names(test_all)
test_select=test_all[,-c(1)]
```

Create another benchmark experiment to find the best regression learner on the train_Select dataset

```{r}
#create task

# we use ksvm to do the training
final.task=makeRegrTask(data=train_select,target = "SalePrice")
final.learners=list(makeLearner("regr.gbm"),makeLearner("regr.randomForest"),makeLearner("regr.nnet"),makeLearner("regr.glmnet"),makeLearner("regr.cvglmnet"),makeLearner("regr.earth"),makeLearner("regr.ksvm"))
final.learners
final.measures=list(mse,rsq)
final.resam=makeResampleDesc("CV",iters=5)
final.bench=benchmark(learners = final.learners,tasks = final.task,resamplings = final.resam,measures = final.measures)
final.bench
#final training using the glmnet package
final.wrapped.learner=makePreprocWrapperCaret("regr.randomForest",ppc.pca=TRUE,ppc.thresh=0.9)
final.train=mlr::train(learner = final.wrapped.learner,task = final.task)
#predict on the test_select data
final.predict=predict(final.train,newdata = test_select)
final.predict
```


Make the second submission

```{r}
submit1=read_csv("sample_submission.csv")
submit1$SalePrice=NULL
submit1$SalePrice=final.predict$data$response
write.csv(submit1,"submit7.csv",row.names = FALSE)
```








