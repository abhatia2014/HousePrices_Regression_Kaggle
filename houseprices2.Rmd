
---
title: House Prices - Advanced Regression Techniques2
output: html_notebook
---

load the required packages first

```{r}
library(mlr,quietly = TRUE)
library(dplyr,quietly = TRUE)
library(ggplot2,quietly = TRUE)
library(caret,quietly = TRUE)
library(purrr,quietly = TRUE)
library(data.table,quietly = TRUE,warn.conflicts = FALSE)
library(readr,quietly = TRUE,warn.conflicts = FALSE)
```

Load the test and train datasets

```{r}
train=read_csv("train.csv")
test=read_csv("test.csv")
```

Rbind the two datasets to perform feature engineering, will later separate the two datasets again. Remove the variable Sale Price from the training set

```{r}
SalePrice=train$SalePrice
train$SalePrice=NULL
train_test=rbind(train,test)
```

Find the str of the combined train_test dataset. Also take the summary

```{r}
str(train_test)
#MSSubClass, OverallQuality, OverallCondition, YearBuilt,YrRemodAdd, GarageYrBlt, MoSold, YrSold are numeric variables that must be converted to factor
summary(train_test)
```

Take all 'Character' class types and convert to factor (categorical) types. Add other numeric variables identified above and convert to factor

```{r}
trn_tst_chr=train_test %>% 
  select_if(is.character) %>% 
  mutate_each(funs(as.factor))
trn_tst_chr$MSSubClass=as.factor(train_test$MSSubClass)
trn_tst_chr$OverallQual=as.factor(train_test$OverallQual)
trn_tst_chr$OverallCond=as.factor(train_test$OverallCond)
trn_tst_chr$MoSold=as.factor(train_test$MoSold)
str(trn_tst_chr)
```

find the number of NAs in each of the factor variables

```{r}
colSums(sapply(trn_tst_chr,is.na))
```

Variables Alley, FireplaceQu, PoolQC, Fence, MiscFeature have very high NAs and are removed

```{r}
trn_tst_chr$Alley=NULL
trn_tst_chr$FireplaceQu=NULL
trn_tst_chr$PoolQC=NULL
trn_tst_chr$Fence=NULL
trn_tst_chr$MiscFeature=NULL
# re-run the command for finding total NAs in the factor variables
colSums(sapply(trn_tst_chr,is.na))
```

Perform imputation of missing factor levels using mode. Use 'Impute' function from mlr package 

```{r}
imp_fctr=impute(trn_tst_chr,classes = list(factor=imputeMode()))
colSums(sapply(imp_fctr$data,is.na))
trn_tst_chr=imp_fctr$data
head(trn_tst_chr,10)
colSums(sapply(trn_tst_chr,is.na))
#no missing values now
```

Take all numerics variables separately 

```{r}
train_test$MSSubClass=as.factor(train_test$MSSubClass)
train_test$OverallCond=as.factor(train_test$OverallCond)
train_test$OverallQual=as.factor(train_test$OverallQual)
train_test$MoSold=as.factor(train_test$MoSold)
trn_tst_num=sapply(train_test,is.numeric)
trn_tst_numeric=train_test[,trn_tst_num]
#convert year variables to age and remove year variables
library(lubridate)
trn_tst_numeric$builtage=as.numeric(year(today())-trn_tst_numeric$YearBuilt)
trn_tst_numeric$YearBuilt=NULL
trn_tst_numeric$remodelledage=as.numeric(year(today())-trn_tst_numeric$YearRemodAdd)
trn_tst_numeric$YearRemodAdd=NULL
trn_tst_numeric$garageage=as.numeric(year(today())-trn_tst_numeric$GarageYrBlt)
trn_tst_numeric$GarageYrBlt=NULL
trn_tst_numeric$soldAge=as.numeric(year(today())-trn_tst_numeric$YrSold)
trn_tst_numeric$YrSold=NULL
```

Find the number of NAs in the numeric variables

```{r}
colSums(sapply(trn_tst_numeric,is.na))
```

There are not too many columns with many NAs. We can impute the missing values by a measure of centrality - mean or median. For the lot Frontage, there are 486 missing values(~25%)- these have to be imputed as a function of Lot Area. For missing values in Garage age- those will be 0 as they represent houses with no garages. For the remaining, we impute with the median

```{r}
#first GarageAge
garageagena=is.na(trn_tst_numeric$garageage)
trn_tst_numeric[garageagena,"garageage"]= 0
sum(is.na(trn_tst_numeric$garageage))
#next we impute Lot Frontage as a regression linear model function of Lot Area
imp_lotfront=impute(trn_tst_numeric, cols = list(LotFrontage=imputeLearner(makeLearner("regr.ksvm"),features = "LotArea"),MasVnrArea=imputeMedian(),BsmtFinSF1=imputeMedian(),BsmtFinSF2=imputeMedian(),BsmtUnfSF=imputeMedian(),
                                              TotalBsmtSF=imputeMedian(),BsmtFullBath=imputeMedian(),BsmtHalfBath=imputeMedian(),GarageCars=imputeMedian(),GarageArea=imputeMedian()))

imp_lotfront$desc
head(imp_lotfront$data,10)
# again find the total NAs in each variable
colSums(sapply(imp_lotfront$data,is.na))
trn_tst_numeric=imp_lotfront$data
colSums(sapply(trn_tst_numeric,is.na))
# no NAs left in the model
```


Check and remove any constant features (min variation) as they don't help in machine learning, The percentage of variabilitiy is kept at 5% meaning all features with below 5% variability are removed

```{r}
trn_tst_numeric=removeConstantFeatures(trn_tst_numeric,perc = 0.05)

```

5 variables with low variance were removed. 
LowQualFinSF,KitchenAbvGr,X3SsnPorch,PoolArea,MiscVal
Next visualize intra feature correlation. Load package corrplot

```{r}
library(corrplot)

numcorr=cor(trn_tst_numeric)
corrplot(numcorr)
```

Find variables of high correlation with each other (multicollinearity) and remove them. Use findCorrelation function from Caret package. All variables with more than 70% correlation are flagged and removed

```{r}
highcorvar=findCorrelation(numcorr,cutoff = 0.70)
# 3 Variables have high multicollinearity
names(trn_tst_numeric[,highcorvar])
# "GrLivArea"    "X1stFlrSF"    "GarageCars"  are the variables with high multicollinearity
trn_tst_numeric=trn_tst_numeric[,-highcorvar]
```

combine numeric, and factordatasets back together using cbind, remove other datasets that are no longer needed

```{r}
train_test_mod=cbind(trn_tst_numeric,trn_tst_chr)
train_test_mod=data.frame(train_test_mod)
sum(is.na(train_test_mod))
#no missing values
rm(trn_tst_chr,trn_tst_num,trn_tst_numeric,train_test,numcorr,highcorvar,imp_fctr,imp_num,YearBuilt)

```

Separate out the train and test sets. Add the response variable 'SalePrice back to training dataset

```{r}
mod_train=train_test_mod[train_test_mod$Id<1461,]
mod_test=train_test_mod[train_test_mod$Id>1460,]
mod_train$SalePrice=SalePrice
mod_train$Id=NULL
mod_test$Id=NULL
```

We now begin with the supervised machine learning tasks. Create a regression task.

```{r}
summary(mod_train$SalePrice)
mod_train$MSSubClass=factor(mod_train$MSSubClass)
mod_test$MSSubClass=factor(mod_test$MSSubClass)
#make the levels for MSSubClass same for training and test sets
mod_train[1417,"MSSubClass"] = 190
levels(mod_train$MSSubClass)
levels(mod_test$MSSubClass)
house_regr=makeRegrTask(data = mod_train,target = "SalePrice")
house_regr
```

Build a benchmark experiment for selecting the best learner (algorithm). List all learners that can perform regression learning on this task

```{r}
regr_learners=listLearners(house_regr)
```

Select 7 different learners for benchmarking experiment

```{r}
house_learn=list( makeLearner("regr.gbm",fix.factors.prediction = TRUE),makeLearner("regr.rpart",fix.factors.prediction = TRUE),makeLearner("regr.randomForest",fix.factors.prediction = TRUE), makeLearner("regr.ksvm",fix.factors.prediction = TRUE), makeLearner("regr.nnet",fix.factors.prediction = TRUE))
house_learn
```

Set the resampling strategy- we use cross validation with 3 iterations

```{r}
house_resamp=makeResampleDesc(method = "CV",iters=3)
house_resamp
```

list all measure that should be evaluated for selecting the best learner. We'll use R Squared and MSE for evaluation purposes

```{r}
listMeasures(house_regr)
house_measures=list(mse,rsq)
house_measures
```

Perform the benchmarking experiment to understand the best learners

```{r}
house_bench=benchmark(learners = house_learn,tasks = house_regr,resamplings = house_resamp,measures = house_measures)
house_bench
```

Visualize results of the benchmarking experiment

```{r}
bench_data=getBMRPerformances(house_bench,as.df = TRUE)
bench_data
plotBMRBoxplots(house_bench,measure = rsq)+aes(col=learner.id)
plotBMRSummary(house_bench,measure = rsq)
```

clearly support vector machine (ksvm) and random forest are the best algorithms giving an rsquared values of almost 85%

We'll now build a prediction model using the Support Vector Machine  algorithm. In first iteration, we'll build the model using the default hyperparameters. Subsequently, we'll do hyperparameter tuning

```{r}

svm_learn=makeLearner("regr.ksvm")
#increase the number of iterations to 5, keep the resampling strategy as cross validation
svm_resamp=makeResampleDesc(method = "CV",iters=5)
svm_measures=list(mse,rsq)
house_trn1=mlr::train(learner = svm_learn,task = house_regr)
house_trn1
```

Predict using the trained model using the new test dataset

```{r}
row.names(mod_test)=NULL
svm_predict1=predict(house_trn1,newdata = mod_test)
head(svm_predict1,20)
```

Make the second submission

```{r}
submit1=read_csv("sample_submission.csv")
submit1$SalePrice=NULL
submit1$SalePrice=svm_predict1$data$response
write.csv(submit1,"submit3.csv",row.names = FALSE)
```

Find variable importance





